---
layout: post
title: Spark, Dataframes, PostgreSQL
date: '2018-08-20T20:21:00.000+05:30'
author: Neil Ghosh
tags:
- cluster
- spark
- bigdata
- dataframe
- postgresql
modified_time: '2018-11-14T20:26:03.030+05:30'
blogger_id: tag:blogger.com,1999:blog-6081677503074893817.post-1406406882409777786
blogger_orig_url: https://www.neilghosh.com/2018/08/spark-dataframes-postgresql.html
---

<div dir="ltr" style="text-align: left;" trbidi="on">Spark is one of the most successful projects at Apache with one of the most popular skills for Big data engineers and a lot of companies look out for this specific skills while hiring.<br /><br />Spark is a distributed computing software where it can employ multiple machines (cluster) which means you can scale horizontally (scale out) by adding more and more computers instead of having to buy/rent computers with higher CPU and Memory (scaling vertically/ scaling up ).<br /><br />Setting up (Standalone Mode)<br /><blockquote class="tr_bq"><ol><li><div>brew install apache-spark</div></li><li><div>Run master : /usr/local/Cellar/apache-spark/2.3.1/bin/spark-class org.apache.spark.deploy.master.Master</div></li><li><div>Run  Slave(s) : /usr/local/Cellar/apache-spark/2.3.1/bin/spark-class  org.apache.spark.deploy.worker.Worker&nbsp;&nbsp;spark://<master_ip>:7077 -c  1 -m 512M&nbsp; </master_ip></div></li></ol></blockquote><blockquote class="tr_bq"><ol style="text-align: left;"><ul><li><div>you will get the master url in the console output after running step 1 and you can run slaves either in another terminal or on another computer which is connected to the same network.</div></li></ul></ol></blockquote><blockquote class="tr_bq"><ol style="text-align: left;"><ol><li><div></div></li></ol><li><div>Run example on master :&nbsp;/usr/local/Cellar/apache-spark/2.3.1/bin/run-example SparkPi</div></li></ol></blockquote>You can run a program by submitting the jar file of the code to spark<br /><br /><blockquote class="tr_bq">usr/local/Cellar/apache-spark/2.3.1/bin/spark-submit â€”class&nbsp;<span data-mce-style="color: #4ec9b0;" style="color: #4ec9b0;">WordCountTask&nbsp;</span>&nbsp;--master  local[2]  /Users/dev/spark/spark-example/target/first-example-1.0-SNAPSHOT.jar   /Users/dev/spark/spark-example/src/test/resources/loremipsum.txt&nbsp;/Users/dev/spark/spark-example/src/test/resources/</blockquote><br />Primarily you can deploy scala or python code in a spark cluster. You can get an interactive shell to try out bits of code and for production, the code is generally shipped (submitting spark job) to the cluster.<br /><br />If you are already familiar with Python dataframes, <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">spark data frames</a> are very similar, you can slide and dice two dimentional data using dataframes.<br /><br />One can directly load the CSV file into a spark dataframe or just can load CSV file into PostgreSQL from where data frame can be loaded using SQL queries.<br /><blockquote class="tr_bq"><div>COPY MyTable FROM '/Users/myUser/myFile.csv' DELIMITER ',' CSV HEADER;</div></blockquote><div>or</div><blockquote class="tr_bq"><div>\copy&nbsp; MyTable FROM '/usr/myFile.csv'&nbsp;&nbsp;DELIMITER ',' CSV HEADER;</div></blockquote><div>or</div><blockquote class="tr_bq"><div>CREATE EXTERNAL TABLE IF NOT EXISTS MyTable(</div><blockquote class="tr_bq"><div>itemId STRING,</div><div>itemCount INT )</div><div>COMMENT 'MyTable'</div><div>ROW FORMAT DELIMITED</div><div>FIELDS TERMINATED BY ','</div><div>STORED AS TEXTFILE</div><div>LOCATION '/user/myFile'</div><div>&nbsp;&nbsp;TBLPROPERTIES('skip.header.line.count'='1');</div></blockquote></blockquote>To deal with PostgreSQL you would need the driver library in the cluster. So you can mention the library while starting the spark shell.<br /><blockquote class="tr_bq">pyspark --conf  spark.executor.extraClassPath=/home/sshuser/postgresql-42.2.4.jar&nbsp;&nbsp;--driver-class-path  /home/sshuser/postgresql-42.2.4.jar --jars  /home/sshuser/postgresql-42.2.4.jar </blockquote>If you want to start a spark shell on your local computer but if the master is running somewhere else<br /><blockquote class="tr_bq">pyspark  --conf spark.executor.extraClassPath=<jdbc .jar="">  --driver-class-path <jdbc .jar=""> --jars <jdbc .jar=""> --master  <master-url></master-url></jdbc></jdbc></jdbc></blockquote>For example<br /><blockquote class="tr_bq">pyspark --conf  spark.executor.extraClassPath=/Users/postgresql-42.2.4.jar&nbsp;&nbsp;--driver-class-path  /Users/postgresql-42.2.4.jar&nbsp;&nbsp;--master <a data-mce-href="spark://192.168.1.199:7077" href="https://www.blogger.com/null">spark://192.168.1.199:7077</a> --executor-memory 512m</blockquote>&nbsp;Although PostgreSQL is RDBMS, you can always use JSON data structure to store variable schema.<br /><blockquote class="tr_bq"><div>create table <span data-mce-style="tab-size: 4;" style="tab-size: 4;">items_var(id VARCHAR(100)</span><span data-mce-style="tab-size: 4;" style="tab-size: 4;">, attributes jsonb);</span></div><div>insert into items_var values('id1', '{"color": "Red", "style": "stripe"}');</div><div>select attributes -&gt; 'color' from items_var;</div><div>select attributes @&gt; '{"color":"Red"}' from items_var;</div><div>select id, attributes&nbsp;&nbsp;from items_var where attributes @&gt; '{"color":"Red"}';</div></blockquote>Now its time to load the data to dataframe from SQL using spark.<br /><br />Load Data from PostgreSQL to dataframe<br /><blockquote class="tr_bq"><div>df = spark.read \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.format("jdbc") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("driver", "org.postgresql.Driver") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("url", "jdbc:postgresql:dbName") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("dbtable", "MyTable") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("user", "myUser") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("password", "") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.load()</div></blockquote>Write data to PostgreSQL<br /><blockquote class="tr_bq"><div></div><div>df.write \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.format("jdbc") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("driver", "org.postgresql.Driver") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("url", "jdbc:postgresql:dbName") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("dbtable", "MyTable") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("user", "myUser") \</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.option("password", "")</div></blockquote>Alternatively<br /><blockquote class="tr_bq"><div></div><div>mode = "overwrite"</div><div>url = "jdbc:postgresql:dbName"</div><div>properties = {"user": "myUser","password": "","driver": "org.postgresql.Driver"}</div><div>df1.write.jdbc(url=url, table="myTable", mode=mode, properties=properties)</div><div><br /></div></blockquote>Perform aggregations<br /><blockquote class="tr_bq"><div></div><div>df.groupBy("itemid").count()</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.filter("count" &gt;= 2)</div><div>&nbsp;&nbsp;&nbsp;&nbsp;.show()</div></blockquote>&nbsp;Most of the time in a production environment you would want to do all these programmatically e.g. from a java based microservice. Check out this <a href="https://github.com/neilghosh/spark-remote-cluster">GitHub repository</a> has some sample code for the same.<br /><br />You can also configure such that pyspark command launches Jupytr notebook and you can interactively run spark commands.<br /></div>